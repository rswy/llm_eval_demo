from llm_eval_package.metrics.base import BaseMetric # Updated import path

class CompletenessMetric(BaseMetric):
    """
    A metric to evaluate the completeness of LLM output compared to a reference answer.
    This is a placeholder and would require a more sophisticated NLP model for actual implementation.
    """

    def __init__(self):
        """
        Initializes the CompletenessMetric.
        """
        super().__init__("Completeness") # Correctly pass the name to the base class

    def compute(self, llm_output: str, reference_answer: str = None, query: str = None, **kwargs) -> float:
        """
        Computes a completeness score.
        For demonstration, this is a placeholder. A real implementation would involve
        comparing key information points or entities.

        Args:
            llm_output (str): The output generated by the LLM.
            reference_answer (str): The human-written reference answer.
            query (str, optional): The user's input query (not used by this metric).
            **kwargs: Additional keyword arguments (not used by this metric).

        Returns:
            float: A placeholder score (e.g., based on length or a simple keyword match).
        """
        if not llm_output or not reference_answer:
            return 0.0

        # Placeholder logic: simple word overlap ratio
        llm_words = set(llm_output.lower().split())
        ref_words = set(reference_answer.lower().split())

        if not ref_words:
            return 1.0 # If reference is empty, consider LLM output complete (or handle as error)

        common_words = llm_words.intersection(ref_words)
        score = len(common_words) / len(ref_words)
        return score

    def get_score_description(self, score: float) -> str:
        """
        Returns a description for the given completeness score.

        Args:
            score (float): The completeness score.

        Returns:
            str: Description of the score.
        """
        if score >= 0.9:
            return "Excellent completeness: The LLM output covers almost all essential information from the reference."
        elif score >= 0.7:
            return "Good completeness: The LLM output covers most essential information from the reference."
        elif score >= 0.5:
            return "Moderate completeness: The LLM output covers some, but not all, essential information from the reference."
        else:
            return "Low completeness: The LLM output misses significant essential information from the reference."

