# src/metrics/fluency_similarity.py

from .utils import safe_word_tokenize # Assuming this utility is still relevant for other metrics
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.translate.meteor_score import single_meteor_score
from rouge_score import rouge_scorer
import warnings



from llm_eval_package.metrics.base import BaseMetric # Updated import path
from sentence_transformers import SentenceTransformer, util
import numpy as np
import streamlit as st # Keep st import for potential error messages within the class

class SemanticSimilarityMetric(BaseMetric):
    """
    A metric to evaluate the semantic similarity between LLM output and a reference answer.
    Uses Sentence-BERT for embedding and cosine similarity.
    """

    def __init__(self, model_path: str):
        """
        Initializes the SemanticSimilarityMetric with a Sentence-BERT model.

        Args:
            model_path (str): The path to the pre-trained Sentence-BERT model.
        """
        super().__init__("Semantic Similarity")
        try:
            self.model = SentenceTransformer(model_path)
            print(f"DEBUG: SemanticSimilarityMetric model loaded successfully from {model_path}")
        except Exception as e:
            print(f"DEBUG: Failed to load Sentence-BERT model from {model_path}. Error: {e}")
            self.model = None # Set model to None to prevent further errors
            # Re-raise the exception to be caught by the Evaluator's __init__
            raise e 

    def compute(self, llm_output: str, reference_answer: str = None, query: str = None, **kwargs) -> float:
        """
        Computes the semantic similarity score between LLM output and reference answer.

        Args:
            llm_output (str): The output generated by the LLM.
            reference_answer (str): The human-written reference answer.
            query (str, optional): The user's input query (not used by this metric).
            **kwargs: Additional keyword arguments (not used by this metric).

        Returns:
            float: The cosine similarity score between the embeddings of the two texts.
                   Returns 0.0 if the model is not loaded or inputs are invalid.
        """
        if self.model is None:
            print("DEBUG: SemanticSimilarityMetric model is not loaded, returning 0.0")
            return 0.0 # Return 0 if model failed to load

        if not llm_output or not reference_answer:
            print("DEBUG: SemanticSimilarityMetric received empty llm_output or reference_answer, returning 0.0")
            return 0.0 # Cannot compute similarity with empty strings

        try:
            # Encode the sentences to get their embeddings
            embedding_llm = self.model.encode(llm_output, convert_to_tensor=True)
            embedding_ref = self.model.encode(reference_answer, convert_to_tensor=True)

            # Compute cosine similarity using util.cos_sim (corrected function name)
            cosine_similarity = util.cos_sim(embedding_llm, embedding_ref)
            score = cosine_similarity.item() # .item() extracts the scalar value from the tensor
            print(f"DEBUG: SemanticSimilarityMetric computed score: {score}")
            return score
        except Exception as e:
            print(f"DEBUG: Error computing semantic similarity: {e}")
            # Do not use st.error here as this is called per row during evaluation
            return 0.0

    def get_score_description(self, score: float) -> str:
        """
        Returns a description for the given semantic similarity score.

        Args:
            score (float): The semantic similarity score.

        Returns:
            str: Description of the score.
        """
        if score >= 0.9:
            return "Excellent semantic similarity: The LLM output is highly similar in meaning to the reference answer."
        elif score >= 0.75:
            return "Good semantic similarity: The LLM output is largely similar in meaning to the reference answer."
        elif score >= 0.5:
            return "Moderate semantic similarity: Some similarity in meaning, but there might be notable differences."
        else:
            return "Low semantic similarity: The LLM output's meaning significantly deviates from the reference answer."




class BleuMetric(BaseMetric):
    """Computes BLEU score for a single reference and prediction using NLTK."""
    def compute(self, references, predictions, **kwargs):
        ref_str = str(references) if references is not None else ""
        pred_str = str(predictions) if predictions is not None else ""

        ref_tokens = [safe_word_tokenize(ref_str)] 
        pred_tokens = safe_word_tokenize(pred_str)

        if not pred_tokens and not ref_tokens[0]:
            return {"bleu": 1.0}
        if not pred_tokens or not ref_tokens[0]:
            warnings.warn("NLTK BLEU: Empty prediction or reference. Assigning BLEU score of 0.")
            return {"bleu": 0.0}
            
        try:
            score = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=SmoothingFunction().method7)
        except Exception as e:
            warnings.warn(f"Could not compute NLTK BLEU for prediction: '{pred_str}'. Error: {e}. Assigning 0.")
            score = 0.0
        return {"bleu": score}

class RougeMetric(BaseMetric):
    """Computes ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L) for a single reference and prediction."""
    def __init__(self):
        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

    def compute(self, references, predictions, **kwargs):
        ref_str = str(references) if references is not None else ""
        pred_str = str(predictions) if predictions is not None else ""

        if not pred_str.strip() and not ref_str.strip():
            return {"rouge_1": 1.0, "rouge_2": 1.0, "rouge_l": 1.0}
        if not pred_str.strip() or not ref_str.strip():
            warnings.warn("ROUGE: Empty prediction or reference. Assigning ROUGE scores of 0.")
            return {"rouge_1": 0.0, "rouge_2": 0.0, "rouge_l": 0.0}
        
        try:
            scores = self.scorer.score(ref_str, pred_str)
            rouge1_score = scores['rouge1'].fmeasure
            rouge2_score = scores['rouge2'].fmeasure
            rougeL_score = scores['rougeL'].fmeasure
        except Exception as e:
            warnings.warn(f"Could not compute ROUGE for prediction: '{pred_str}'. Error: {e}. Assigning 0.")
            rouge1_score, rouge2_score, rougeL_score = 0.0, 0.0, 0.0
        
        return {
            "rouge_1": rouge1_score,
            "rouge_2": rouge2_score,
            "rouge_l": rougeL_score
        }

class MeteorMetric(BaseMetric):
    """Computes METEOR score for a single tokenized reference and prediction using NLTK."""
    def compute(self, references, predictions, **kwargs):
        ref_str = str(references) if references is not None else ""
        pred_str = str(predictions) if predictions is not None else ""

        ref_tokens = safe_word_tokenize(ref_str)
        pred_tokens = safe_word_tokenize(pred_str)

        if not pred_tokens and not ref_tokens:
             return {"meteor": 1.0}
        if not pred_tokens or not ref_tokens:
            warnings.warn("NLTK METEOR: Empty token list for prediction or reference. Assigning METEOR score of 0.")
            return {"meteor": 0.0}
            
        try:
            score = single_meteor_score(ref_tokens, pred_tokens) # NLTK's single_meteor_score expects tokenized strings
        except Exception as e:
            warnings.warn(f"Could not compute NLTK METEOR for prediction tokens: '{pred_tokens}'. Error: {e}. Assigning 0.")
            score = 0.0
        
        return {"meteor": score}
