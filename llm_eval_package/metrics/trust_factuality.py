from llm_eval_package.metrics.base import BaseMetric # Updated import path

class TrustFactualityMetric(BaseMetric):
    """
    A metric to evaluate the trust and factuality of LLM output against a reference answer.
    This is a placeholder and would require a sophisticated NLP model or external knowledge base.
    """

    def __init__(self):
        """
        Initializes the TrustFactualityMetric.
        """
        super().__init__("Trust & Factuality") # Correctly pass the name to the base class

    def compute(self, llm_output: str, reference_answer: str = None, query: str = None, **kwargs) -> float:
        """
        Computes a trust and factuality score.
        For demonstration, this is a placeholder. A real implementation would involve
        cross-referencing facts, checking for hallucinations, or using NLI models.

        Args:
            llm_output (str): The output generated by the LLM.
            reference_answer (str): The human-written reference answer (used as ground truth for factuality).
            query (str, optional): The user's input query (not used by this metric).
            **kwargs: Additional keyword arguments (not used by this metric).

        Returns:
            float: A placeholder score (e.g., based on simple keyword presence).
        """
        if not llm_output or not reference_answer:
            return 0.0 # Cannot assess factuality without both

        # Placeholder logic: simple check if reference answer content is present in LLM output.
        # This is a very weak proxy for factuality.
        reference_lower = reference_answer.lower()
        llm_output_lower = llm_output.lower()

        # Check if a significant portion of the reference answer's unique words are in the LLM output
        ref_words = set(reference_lower.split())
        llm_words = set(llm_output_lower.split())

        if not ref_words:
            return 1.0 # If reference is empty, consider LLM output factual (or handle as error)

        common_words = ref_words.intersection(llm_words)
        score = len(common_words) / len(ref_words)
        return score

    def get_score_description(self, score: float) -> str:
        """
        Returns a description for the given trust and factuality score.

        Args:
            score (float): The trust and factuality score.

        Returns:
            str: Description of the score.
        """
        if score >= 0.9:
            return "Excellent trust and factuality: The LLM output is highly consistent with factual information from the reference."
        elif score >= 0.75:
            return "Good trust and factuality: The LLM output is largely consistent with factual information from the reference."
        elif score >= 0.5:
            return "Moderate trust and factuality: Some factual inconsistencies or deviations from the reference might be present."
        else:
            return "Low trust and factuality: The LLM output contains significant factual errors or hallucinations compared to the reference."

