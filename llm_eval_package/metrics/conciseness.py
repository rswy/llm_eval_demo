from llm_eval_package.metrics.base import BaseMetric # Updated import path

class ConcisenessMetric(BaseMetric):
    """
    A metric to evaluate the conciseness of LLM output.
    This is a placeholder and would require a more sophisticated NLP model for actual implementation.
    """

    def __init__(self):
        """
        Initializes the ConcisenessMetric.
        """
        super().__init__("Conciseness") # Correctly pass the name to the base class

    def compute(self, llm_output: str, reference_answer: str = None, query: str = None, **kwargs) -> float:
        """
        Computes a conciseness score.
        For demonstration, this is a placeholder. A real implementation might assess
        redundancy, verbosity, or directness.

        Args:
            llm_output (str): The output generated by the LLM.
            reference_answer (str, optional): The human-written reference answer (not directly used for conciseness).
            query (str, optional): The user's input query (not used by this metric).
            **kwargs: Additional keyword arguments (not used by this metric).

        Returns:
            float: A placeholder score (e.g., inverse of word count relative to a target).
        """
        if not llm_output:
            return 1.0 # Empty output is perfectly concise (or handle as per policy)

        # Placeholder logic: shorter is more concise, up to a point.
        # This is a very simplistic approach.
        word_count = len(llm_output.split())
        # Assume an ideal word count for a concise answer, e.g., 20 words.
        # This would need to be context-dependent in a real scenario.
        ideal_word_count = 20

        if word_count <= ideal_word_count:
            return 1.0 # Perfectly concise if within ideal limit
        else:
            # Score decreases as word count increases beyond ideal
            # Example: 1 - (excess_words / ideal_word_count)
            score = max(0.0, 1.0 - ((word_count - ideal_word_count) / ideal_word_count))
            return score

    def get_score_description(self, score: float) -> str:
        """
        Returns a description for the given conciseness score.

        Args:
            score (float): The conciseness score.

        Returns:
            str: Description of the score.
        """
        if score >= 0.9:
            return "Excellent conciseness: The LLM output is brief and to the point."
        elif score >= 0.7:
            return "Good conciseness: The LLM output is mostly concise with minor verbosity."
        elif score >= 0.5:
            return "Moderate conciseness: The LLM output contains some unnecessary verbosity."
        else:
            return "Low conciseness: The LLM output is overly verbose or redundant."

