import streamlit as st
import pandas as pd

class TutorialView:
    """
    Manages the display of the tutorial/how-to-use section in the Streamlit application.
    """

    def __init__(self):
        """
        Initializes the TutorialView.
        """
        pass

    def render_tutorial(self):
        """
        Renders the tutorial content.
        """
        st.header("Welcome:)")
        st.markdown(
            """
            This tool helps you evaluate the performance of your Large Language Models (LLMs)
            based on various natural language processing metrics.
            """
        )

        st.subheader("How to Use:")

        st.markdown("### Step 1: Upload Your Data")
        st.markdown(
            """
            On the left sidebar, use the "Upload your dataset (CSV or JSON)" button to upload your evaluation data.
            Your file should contain the following columns:
            - `query`: The input prompt or question given to the LLM.
            - `llm_output`: The response generated by your LLM.
            - `reference_answer`: The human-written or ground-truth answer.
            - `test_description` (Optional): A brief description of the test case.
            - `test_config` (Optional): A configuration or category for the test case, useful for filtering or grouping results.
            """
        )
        st.info("Example CSV/JSON structure:")
        
        example_data = {
            'query': [
                'What is the capital of France?',
                'Summarize the plot of Hamlet.',
                'Is the sky blue?'
            ],
            'llm_output': [
                'Paris is the capital.',
                'Hamlet is about a prince seeking revenge.',
                'Yes, the sky is typically blue.'
            ],
            'reference_answer': [
                'The capital of France is Paris, known for its Eiffel Tower.',
                'Hamlet, Prince of Denmark, seeks revenge on his uncle Claudius for murdering his father and marrying his mother.',
                'Under normal atmospheric conditions, the sky appears blue due to Rayleigh scattering.'
            ],
            'test_description': [
                'Basic factual question',
                'Literary summarization',
                'Simple true/false question'
            ],
            'test_config': [
                'Geography',
                'Literature',
                'General Knowledge'
            ]
        }
        example_df = pd.DataFrame(example_data)
        st.dataframe(example_df, use_container_width=True)


        st.markdown("### Step 2: Select Task Type <Developer Version Only>")
        st.markdown(
            """
            Choose the task type that best describes your LLM's function (e.g., RAG FAQ, Classification, Summarization).
            This helps in preselecting relevant metrics.
            """
        )

        st.markdown("### Step 3: Select Metrics <Developer Version Only>")
        st.markdown(
            """
            From the sidebar, select the metrics you want to use for evaluation.
            Based on your chosen task type, some metrics might be pre-selected for your convenience.
            You can add or remove metrics as needed.
            """
        )

        st.markdown("### Step 4: Set Thresholds (Optional)")
        st.markdown(
            """
            You can use the default thresholds for each metric, or toggle "Use Custom Thresholds"
            to define your own pass/fail cut-off values for each selected metric.
            """
        )
        st.markdown(
            """
            **Note on Safety Metric:** If you select the "Safety" metric, a text area will appear
            where you can input comma-separated keywords that you consider sensitive or unsafe.
            The LLM output will be flagged as "Fail" for Safety if any of these keywords are detected.
            """
        )

        st.markdown("### Step 5: Run Evaluation")
        st.markdown(
            """
            Once your data is uploaded and metrics are selected, click the "Run Evaluation" button
            in the sidebar. The tool will process your data and calculate scores for each metric.
            """
        )

        st.markdown("### Step 6: View Results")
        st.markdown(
            """
            After the evaluation completes, the main dashboard will display a summary report
            with pass rates and average scores for each metric, along with a detailed table
            showing individual scores for each test case. You can also download the full results.
            """
        )

        st.markdown("---")
        st.markdown("Enjoy evaluating your LLMs!")

