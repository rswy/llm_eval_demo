import os

# Define task types
TASK_TYPE_RAG_FAQ = "rag_faq"
TASK_TYPE_CLASSIFICATION = "classification"
TASK_TYPE_SUMMARIZATION = "summarization"
TASK_TYPE_GENERIC = "generic" # For tasks that don't fit specific categories

# Mapping of task types to their display names
TASK_TYPE_MAPPING = {
    TASK_TYPE_RAG_FAQ: "RAG FAQ",
    TASK_TYPE_CLASSIFICATION: "Classification",
    TASK_TYPE_SUMMARIZATION: "Summarization",
    TASK_TYPE_GENERIC: "Generic Text Generation"
}

# Define available metrics and their corresponding classes
# This dictionary maps a metric name (string) to its class reference
# The actual metric classes are imported in evaluator.py
AVAILABLE_METRICS = {
    "Semantic Similarity": "SemanticSimilarityMetric",
    "Completeness": "CompletenessMetric",
    "Conciseness": "ConcisenessMetric",
    "Trust & Factuality": "TrustFactualityMetric",
    "Safety": "SafetyMetric", # Added Safety Metric
    # "Fluency": "FluencyMetric", # Placeholder for future implementation
    # "Coherence": "CoherenceMetric", # Placeholder for future implementation
    # "Toxicity": "ToxicityMetric", # Placeholder for future implementation
    # "Bias": "BiasMetric", # Placeholder for future implementation
    # "Accuracy": "AccuracyMetric", # Placeholder for future implementation
}

# Preselection of metrics for specific tasks
TASK_METRICS_PRESELECTION = {
    TASK_TYPE_RAG_FAQ: ["Semantic Similarity"], # Changed default to only Semantic Similarity
    # Add other task type preselection here if needed
    # TASK_TYPE_CLASSIFICATION: ["Accuracy"],
}


# Default thresholds for each metric
# These thresholds are used to determine pass/fail status for each metric
METRIC_THRESHOLDS = {
    "Semantic Similarity": 0.75, # Cosine similarity score
    "Completeness": 0.70,       # Percentage of reference covered
    "Conciseness": 0.80,        # Ratio of concise words
    "Trust & Factuality": 0.75, # Factual consistency score
    "Safety": 1.0,              # 1.0 for safe, 0.0 for unsafe
}

# Required columns for the input CSV/JSON file
# 'query': The user's input query
# 'llm_output': The output generated by the LLM
# 'reference_answer': The human-written reference answer
# 'test_description': (Optional) A description for the test case
# 'test_config': (Optional) Configuration or category of the query for filtering/grouping
REQUIRED_COLUMNS = [
    'query',
    'llm_output',
    'reference_answer',
    'test_description', # Added optional column
    'test_config' # Changed from 'category'
]

# Columns that are typically for internal reference or raw data,
# and can be hidden from the primary detailed results table by default for cleaner view.
DEFAULT_HIDDEN_COLUMNS_IN_RESULTS = [
    'ref_facts',
    'ref_key_points'
]

# Define the directory for models
# This path is relative to the project root
# It assumes 'llm_eval_package' is directly under the project root
MODEL_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'models')

# Configuration for the Sentence-BERT model
# This model is used for semantic similarity calculations
SENTENCE_BERT_MODEL = "all-MiniLM-L6-v2"
SENTENCE_BERT_MODEL_PATH = os.path.join(MODEL_DIR, SENTENCE_BERT_MODEL)

# Interpretation engine configuration
# This could include rules or prompts for generating insights
INTERPRETATION_CONFIG = {
    "semantic_similarity_insight": "Semantic similarity measures how close the meaning of the LLM's output is to the reference answer. A higher score indicates better understanding and relevance.",
    "completeness_insight": "Completeness assesses whether the LLM's output covers all essential information present in the reference answer. A higher score means more comprehensive answers.",
    "conciseness_insight": "Conciseness evaluates if the LLM's output is brief and to the point without losing necessary information. A higher score indicates less verbosity.",
    "trust_factuality_insight": "Trust and factuality checks if the LLM's output is consistent with factual information provided. A higher score means more reliable and accurate answers.",
    "safety_insight": "Safety checks if the LLM's output contains any user-defined sensitive keywords. A score of 1.0 means no sensitive keywords were detected (safe), while 0.0 means sensitive content was found (unsafe).",
}

# Report generation configuration
REPORT_CONFIG = {
    "title": "LLM Evaluation Report",
    "output_dir": "reports",
    "file_name": "llm_evaluation_report.html",
    "template_path": "templates/report_template.html" # Placeholder for future templating
}

# UI Configuration
UI_CONFIG = {
    "sidebar_title": "LLM Evaluation Tool",
    "upload_section_title": "Upload Data",
    "metrics_section_title": "Select Metrics",
    "run_button_label": "Run Evaluation",
    "results_section_title": "Evaluation Results",
    "tutorial_section_title": "How to Use",
}

# Feature Toggles for UI Sections (Developer controlled)
# Set to True for developers (full control), False for end-users (simplified)
DEVELOPER_MODE = True 

# These flags now depend on DEVELOPER_MODE
ENABLE_TASK_SELECTION = DEVELOPER_MODE # Set to False to hide Step 2
ENABLE_METRIC_SELECTION = DEVELOPER_MODE # Set to False to hide Step 3
