import os

# Define task types
TASK_TYPE_RAG_FAQ = "rag_faq"
TASK_TYPE_CLASSIFICATION = "classification"
TASK_TYPE_SUMMARIZATION = "summarization"
TASK_TYPE_GENERIC = "generic" # For tasks that don't fit specific categories

# Mapping of task types to their display names
TASK_TYPE_MAPPING = {
    TASK_TYPE_RAG_FAQ: "RAG FAQ",
    TASK_TYPE_CLASSIFICATION: "Classification",
    TASK_TYPE_SUMMARIZATION: "Summarization",
    TASK_TYPE_GENERIC: "Generic Text Generation"
}

# Define available metrics and their corresponding classes
# This dictionary maps a metric name (string) to its class reference
# The actual metric classes are imported in evaluator.py
AVAILABLE_METRICS = {
    "Semantic Similarity": "SemanticSimilarityMetric",
    "Fact Adherence": "FactAdherenceMetric",  # <-- ADDED
    # "Trust & Factuality": "TrustFactualityMetric",
    # "Completeness": "CompletenessMetric",
    # "Conciseness": "ConcisenessMetric",
    # "Safety": "SafetyMetric", # Added Safety Metric
    # "Fluency": "FluencyMetric", # Placeholder for future implementation
    # "Coherence": "CoherenceMetric", # Placeholder for future implementation
    # "Toxicity": "ToxicityMetric", # Placeholder for future implementation
    # "Bias": "BiasMetric", # Placeholder for future implementation
    # "Accuracy": "AccuracyMetric", # Placeholder for future implementation
}


# Default thresholds for each metric
# These thresholds are used to determine pass/fail status for each metric
METRIC_THRESHOLDS = {
    "Semantic Similarity": 0.75,
    "Completeness": 0.70,
    "Conciseness": 0.80,
    "Trust & Factuality": 0.75,
    "Fact Adherence": 0.99, # e.g., require all facts to be present (score 1.0 for all found)
    "Safety": 1.0,
}

# --- Overall Pass/Fail Criteria Constants ---
PASS_CRITERION_ALL_PASS = "All selected metrics must pass"
PASS_CRITERION_ANY_PASS = "Any selected metric can pass"
# (Future: PASS_CRITERION_SPECIFIC_METRIC = "A specific primary metric must pass")

DEFAULT_PASS_CRITERION = PASS_CRITERION_ALL_PASS

# Preselection of metrics for specific tasks
TASK_METRICS_PRESELECTION = {
    TASK_TYPE_RAG_FAQ: ["Semantic Similarity"], # Changed default to only Semantic Similarity
    # Add other task type preselection here if needed
    # TASK_TYPE_CLASSIFICATION: ["Accuracy"],
}




# Required columns for the input CSV/JSON file
# 'query': The user's input query
# 'llm_output': The output generated by the LLM
# 'reference_answer': The human-written reference answer
# 'test_description': (Optional) A description for the test case
# 'test_config': (Optional) Configuration or category of the query for filtering/grouping

REQUIRED_COLUMNS = [
    'query',
    'llm_output',
    'reference_answer',
    'initial_reviewer_verdict', # << NEW: Optional, for pre-populating UAT result
    'required_facts',     # Optional, for Fact Adherence metric
    'test_description',   # Optional
    'test_config',         # Optional
]

# Columns that are typically for internal reference or raw data,
# and can be hidden from the primary detailed results table by default for cleaner view.
DEFAULT_HIDDEN_COLUMNS_IN_RESULTS = [
    'ref_facts',
    'ref_key_points'
]

# Define the directory for models
# This path is relative to the project root
# It assumes 'llm_eval_package' is directly under the project root
MODEL_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'models')

# Configuration for the Sentence-BERT model
# This model is used for semantic similarity calculations
SENTENCE_BERT_MODEL = "all-MiniLM-L6-v2"
SENTENCE_BERT_MODEL_PATH = os.path.join(MODEL_DIR, SENTENCE_BERT_MODEL)

# Interpretation engine configuration
# This could include rules or prompts for generating insights

INTERPRETATION_CONFIG = {
    # ... (other metric insights)
    "semantic_similarity_insight": "Semantic similarity measures how close the meaning of the LLM's output is to the reference answer. Higher score = better relevance.",
    "fact_adherence_insight": "Fact Adherence checks if specific, predefined 'required facts' are present in the LLM's output. A score of 1.0 means all required facts were found. This is useful for ensuring critical pieces of information are always included.",
    "completeness_insight": "Completeness assesses if the LLM's output covers essential information from the reference answer. Higher score = more comprehensive.",
    "conciseness_insight": "Conciseness evaluates if the LLM's output is brief and to the point. Higher score = less verbosity.",
    "trust_factuality_insight": "Trust & Factuality checks if the LLM's output is consistent with factual information in the reference. Higher score = more reliable.",
    "safety_insight": "Safety checks for user-defined sensitive keywords. Score 1.0 = safe (no keywords detected), 0.0 = unsafe.",
}

# Report generation configuration
REPORT_CONFIG = {
    "title": "LLM Evaluation Report",
    "output_dir": "reports",
    "file_name": "llm_evaluation_report.html",
    "template_path": "templates/report_template.html" # Placeholder for future templating
}

# UI Configuration
UI_CONFIG = {
    "sidebar_title": "LLM Evaluation Tool",
    "upload_section_title": "Upload Data",
    "metrics_section_title": "Select Metrics",
    "run_button_label": "Run Evaluation",
    "results_section_title": "Evaluation Results",
    "tutorial_section_title": "How to Use",
}

# Feature Toggles for UI Sections (Developer controlled)

# These flags now depend on DEVELOPER_MODE
ENABLE_TASK_SELECTION = False # Set to False to hide Step 2

# Set to True for developers (full control), False for end-users (simplified)
DEVELOPER_MODE = True
ENABLE_METRIC_SELECTION = True # Set to False to hide Step 3
