import sys
import os
from pathlib import Path
from typing import List, Dict, Any, Optional

# Add the project root to sys.path so Python can find 'llm_eval_package'
project_root = os.path.abspath(os.path.dirname(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field

# Import core components from your modularized package
from llm_eval_package.core.engine import Evaluator
from llm_eval_package.config import (
    AVAILABLE_METRICS, METRIC_THRESHOLDS, TASK_TYPE_MAPPING,
    TASK_METRICS_PRESELECTION, REQUIRED_COLUMNS
)

# --- Pydantic Models for API Request/Response Validation and Documentation ---

class TestCaseInput(BaseModel):
    """
    Represents a single test case input for evaluation.
    Matches the structure expected by your DataLoader.
    """
    query: str = Field(..., description="The input prompt or question given to the LLM.")
    llm_output: str = Field(..., description="The response generated by the LLM.")
    reference_answer: str = Field(..., description="The human-written or ground-truth answer.")
    test_description: Optional[str] = Field(None, description="A brief description of the test case.")
    test_config: Optional[str] = Field(None, description="A categorical label for the test case (e.g., 'HR_Policy_FAQ', 'Financial_Product_Info').")
    # Add other optional columns if you want them to be part of the API input
    id: Optional[str] = Field(None, description="Unique identifier for the test case.")
    task_type: Optional[str] = Field(None, description="Task type for the test case (e.g., 'rag_faq', 'summarization').")
    model: Optional[str] = Field(None, description="Name of the LLM model being evaluated.")

class EvaluationRequest(BaseModel):
    """
    Defines the structure of the request body for the /evaluate endpoint.
    """
    test_cases: List[TestCaseInput] = Field(..., description="A list of test cases to be evaluated.")
    selected_metrics: List[str] = Field(
        ["Semantic Similarity"],
        description=f"List of metrics to apply. Available: {', '.join(AVAILABLE_METRICS.keys())}."
    )
    custom_thresholds: Optional[Dict[str, float]] = Field(
        None,
        description="Optional: Custom pass/fail thresholds for metrics (e.g., {'Semantic Similarity': 0.8})."
    )
    sensitive_keywords: Optional[List[str]] = Field(
        None,
        description="Optional: List of sensitive keywords for the 'Safety' metric."
    )

class EvaluationResult(BaseModel):
    """
    Defines the structure of a single evaluation result for a test case.
    This will dynamically include score and pass/fail columns.
    Using Dict[str, Any] for flexibility as columns vary.
    """
    # Using a generic dictionary to allow for dynamic metric columns
    # For more strict validation, you'd define each possible metric score/pass_fail explicitly.
    results: Dict[str, Any] = Field(..., description="Detailed evaluation results for a single test case.")

# --- FastAPI Application Instance ---
app = FastAPI(
    title="LLM Evaluation API",
    description="API for evaluating Large Language Model outputs using various metrics.",
    version="1.0.0",
)

# --- Initialize Evaluator (cached per process by Streamlit's @st.cache_resource, but here it's per API app instance) ---
# For a pure FastAPI app, you might want to manage this caching/singleton pattern differently
# or ensure the Evaluator is initialized only once.
# For simplicity, we'll initialize it globally here.
try:
    evaluator_instance = Evaluator()
except Exception as e:
    print(f"CRITICAL ERROR: Failed to initialize Evaluator: {e}")
    sys.exit(1) # Exit if the core evaluator cannot be initialized

# --- API Endpoints ---

@app.get("/metrics", response_model=Dict[str, Any], summary="Get Available Metrics")
async def get_available_metrics():
    """
    Retrieves a list of all available evaluation metrics and their default thresholds.
    """
    return {
        "available_metrics": list(AVAILABLE_METRICS.keys()),
        "default_thresholds": METRIC_THRESHOLDS
    }

@app.get("/tasks", response_model=Dict[str, Any], summary="Get Available Task Types")
async def get_available_task_types():
    """
    Retrieves a list of all available LLM task types.
    """
    return {
        "task_types": TASK_TYPE_MAPPING
    }

@app.post("/evaluate", response_model=List[EvaluationResult], summary="Run LLM Evaluation")
async def run_evaluation(request: EvaluationRequest):
    """
    Runs the LLM evaluation on the provided test cases using the specified metrics.

    **Input:**
    - `test_cases`: A list of dictionaries, each representing a test case.
      Each test case must include `query`, `llm_output`, `reference_answer`.
      Optional fields: `test_description`, `test_config`, `id`, `task_type`, `model`.
    - `selected_metrics`: A list of metric names to evaluate (e.g., ["Semantic Similarity", "Safety"]).
    - `custom_thresholds` (Optional): A dictionary mapping metric names to custom float thresholds.
    - `sensitive_keywords` (Optional): A list of strings for the 'Safety' metric.

    **Output:**
    - A list of dictionaries, where each dictionary represents an evaluated test case
      and includes original input fields plus new columns for each metric's score
      (e.g., "Semantic Similarity Score") and pass/fail status
      (e.g., "Semantic Similarity Pass/Fail").
    """
    if not request.test_cases:
        raise HTTPException(status_code=400, detail="No test cases provided for evaluation.")
    if not request.selected_metrics:
        raise HTTPException(status_code=400, detail="No metrics selected for evaluation.")

    # Convert list of Pydantic models to pandas DataFrame
    # Ensure all required columns are present, fill missing optional ones with None/empty string
    data_for_df = []
    for tc in request.test_cases:
        tc_dict = tc.dict()
        # Ensure all REQUIRED_COLUMNS from config are present, even if empty
        for col in REQUIRED_COLUMNS:
            if col not in tc_dict:
                tc_dict[col] = None # Or "" if you prefer empty string
        data_for_df.append(tc_dict)

    df_input = pd.DataFrame(data_for_df)

    # Ensure mandatory columns are present after DataFrame creation
    mandatory_cols = ['query', 'llm_output', 'reference_answer']
    if not all(col in df_input.columns for col in mandatory_cols):
        raise HTTPException(status_code=400, detail=f"Missing one or more mandatory columns in test cases. Required: {', '.join(mandatory_cols)}")

    try:
        # Call the evaluator
        df_evaluated = evaluator_instance.evaluate_dataframe(
            df_input,
            request.selected_metrics,
            custom_thresholds=request.custom_thresholds,
            sensitive_keywords=request.sensitive_keywords
        )
        
        # Convert DataFrame back to a list of dictionaries for JSON response
        # Use .to_dict(orient='records') for easy conversion
        # Ensure NaN values are handled (e.g., converted to None or string "NaN")
        results_list = df_evaluated.replace({np.nan: None}).to_dict(orient='records')
        
        # Wrap each dictionary in EvaluationResult model
        return [{"results": item} for item in results_list]

    except Exception as e:
        # Log the detailed error for debugging
        import traceback
        print(f"ERROR during evaluation API call: {e}\n{traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"An error occurred during evaluation: {e}")

# --- Health Check Endpoint (Optional but Recommended) ---
@app.get("/health", summary="Health Check")
async def health_check():
    """
    Checks the health of the API server.
    """
    return {"status": "ok", "message": "LLM Evaluation API is running."}

